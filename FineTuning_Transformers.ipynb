{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3efc92a6",
   "metadata": {},
   "source": [
    "# Fine-Tuning Model For Metaphor Detection: Local Model PEFT.\n",
    "\n",
    "In this notebook, I will walk you through the process how local model is fine-tuned to detect metaphor in our study.\n",
    "\n",
    "This includes model fine-tuning via parameter efficient fine-tuning, and then model inferring.\n",
    "\n",
    "First, let us start with the dependencies. These packages are needed: transformers peft pandas datasets torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9632e6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers peft pandas datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea4b7ea6",
   "metadata": {},
   "source": [
    "Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b879d62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers import TrainingArguments,Trainer\n",
    "from peft import LoraConfig, TaskType, get_peft_model\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00301e7",
   "metadata": {},
   "source": [
    "Before moving into fine-tuning, you'll need to load the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df1a269",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_fp=\"data/metaphor_dataset.csv\"\n",
    "data_df=pd.read_csv(data_fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61703c5",
   "metadata": {},
   "source": [
    "And then, perform the train-test split.\n",
    "\n",
    "This would split the dataset into two independent parts: train set and test set.\n",
    "\n",
    "The train set will be exposed to the model in fine-tuning, which would make the model \"learn\" the traits of metaphor.\n",
    "\n",
    "The test set will remain unexposed to the model during fine-tuning. It will be reserved for evaluation on the performance of model.\n",
    "\n",
    "Here, we use a typical train-test split ratio of 8:2, and to maximize replicability, you may also wish to set a random seed(here seed =1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceddfca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=1\n",
    "train_ratio=0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361a2789",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df=data_df.sample(frac=train_ratio,random_state=seed)\n",
    "test_df=data_df.drop(index=train_df.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af14ead",
   "metadata": {},
   "source": [
    "Load model and tokenizer from huggingface.\n",
    "\n",
    "Please note: some model used in this study is gated. You will need to apply for access. You apply for access is free, and it would typically get approved within 72 hours. Please check your huggingface account for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e53a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tknzid=\"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "modelid=\"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "\n",
    "tokenizer=AutoTokenizer.from_pretrained(tknzid)\n",
    "model=AutoModelForCausalLM.from_pretrained(modelid,torch_dtype=torch.float16,device_map=\"cuda\") # note please refer to your own spec: cuda is for nvidia device.\n",
    "\n",
    "tokenizer.pad_token=tokenizer.eos_token\n",
    "model.pad_token=tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac488505",
   "metadata": {},
   "source": [
    "Wrap the text in train set in chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abdcde1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_msg_0=\"Can you please identify and tag the metaphors in the following text? \"\n",
    "\n",
    "messages_lists=[]\n",
    "for idx in range(0,train_df.shape[0]):\n",
    "    text=train_df.iloc[idx][\"u\"].replace(\"\\n\",\" \")\n",
    "    raw_text=train_df.iloc[idx][\"plain\"].replace(\"\\n\",\" \")\n",
    "    messages=[\n",
    "            {\"role\":\"user\",\"content\":user_msg_0+\"\\n\"+raw_text},\n",
    "            {\"role\":\"assistant\",\"content\":text},\n",
    "        ]\n",
    "    messages_lists.append(messages)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77608f29",
   "metadata": {},
   "source": [
    "Apply chat template and tokenize the texts for fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ecf1a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds=Dataset.from_dict({\"text\":messages_lists})\n",
    "\n",
    "def tokenize_item(item):\n",
    "    item=tokenizer.apply_chat_template(item[\"text\"],tokenize=False)\n",
    "    item=tokenizer(item,padding=True,truncation=True)\n",
    "    item[\"labels\"]=item[\"input_ids\"].copy()\n",
    "    return item\n",
    "\n",
    "ds=ds.map(tokenize_item,batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d436bc",
   "metadata": {},
   "source": [
    "Setup parameter efficient fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd0775c",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "     target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "        \"lm_head\",\n",
    "    ],\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    ")\n",
    "model=get_peft_model(model, peft_config)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"chatbot\",\n",
    "    per_device_train_batch_size=1,\n",
    "    eval_strategy=\"no\",\n",
    "    report_to=\"none\",\n",
    "    fp16=True,\n",
    ")\n",
    "\n",
    "trainer=Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b07ac1cd",
   "metadata": {},
   "source": [
    "Run the following to start fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84a7382",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65504cac",
   "metadata": {},
   "source": [
    "Don't forget to save your model after fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df20f7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"ft_local_llama1b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e364e9",
   "metadata": {},
   "source": [
    "Good. Till now we've fine-tuned the model. The next step: inferring. Let's try our model on some new text(or text in test set)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045b7c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text=test_df.iloc[idx][\"plain\"].replace(\"\\n\",\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04c218b",
   "metadata": {},
   "source": [
    "Infer on fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f70cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_msg_0=\"Can you please identify and tag the metaphors in the following text?\"\n",
    "\n",
    "messages=[\n",
    "        {\"role\":\"user\",\"content\":user_msg_0+\"\\n\"+raw_text},\n",
    "    ]\n",
    "\n",
    "ct=tokenizer.apply_chat_template(messages,tokenize=False)\n",
    "ct=tokenizer(ct,return_tensors=\"pt\",truncation=True)\n",
    "\n",
    "outputs=model.generate(input_ids=ct[\"input_ids\"].to(\"cuda\"),attention_mask=ct[\"attention_mask\"].to(\"cuda\"))\n",
    "\n",
    "out_text=tokenizer.decode(outputs[0])\n",
    "pttn=\"<\\\\|start_header_id\\\\|>assistant<\\\\|end_header_id\\\\|>(.*)\"\n",
    "res=re.search(pttn,out_text,flags=re.DOTALL)\n",
    "if(res==None):\n",
    "    rs=\"\"\n",
    "else:\n",
    "    rs=res.group(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c72277",
   "metadata": {},
   "source": [
    "View the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19144bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "alpha",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
