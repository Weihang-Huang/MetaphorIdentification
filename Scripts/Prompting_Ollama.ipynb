{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb682432",
   "metadata": {},
   "source": [
    "# Metaphor Identification via In Context Learning with Local Model in Ollama\n",
    "\n",
    "In this notebook, I will show the workflow of metaphor identification via in context learning(ICL) with local model. Here, we host local model in Ollama.\n",
    "\n",
    "Let's start with the dependencies. To run this notebook, you need the following packages: ollama, pandas.\n",
    "\n",
    "Run the following script to install these packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "022a0203",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ollama pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb3a5d5e",
   "metadata": {},
   "source": [
    "Import packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4633d5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from ollama import chat\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ef6b12",
   "metadata": {},
   "source": [
    "Next, you'll need to load the prompt(s). The prompt(s) are stored as csv. You can use the following script to load it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0806e2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_fp=\"Resources/prompts.csv\"\n",
    "prompt_df=pd.read_csv(prompt_fp,index_col=0)\n",
    "pids=list(set(prompt_df.index.to_list()))\n",
    "p_strats={}\n",
    "p_strats_info={}\n",
    "for pid in pids:\n",
    "    chat_temp_df=prompt_df.loc[pid]\n",
    "    chat_temp=[]\n",
    "    for i in range(0,chat_temp_df.shape[0]):\n",
    "        item=chat_temp_df.iloc[i]\n",
    "        chat_temp.append({\"role\":item[\"role\"],\"content\":item[\"content\"]})\n",
    "    p_strats[pid]=chat_temp\n",
    "    p_strats_info[pid]=chat_temp_df.iloc[0][\"name\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db38e488",
   "metadata": {},
   "source": [
    "The prompts are stored in the dictionary p_strats. You may access prompt via prompt_id(pid). For a full list of pid:prompt strategy, simply check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2095e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_strats_info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f32a2e29",
   "metadata": {},
   "source": [
    "Select your prompt strategy via prompt id using the folllowing script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a23e298",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_strat=p_strats[pid]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5297cc",
   "metadata": {},
   "source": [
    "After the prompts are loaded, you'll need to load the test text. To load it from our corpus, simply do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985b9c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_fp=\"Corpus/metaphor_dataset.csv\"\n",
    "ds_df=pd.read_csv(ds_fp,index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf4f27b",
   "metadata": {},
   "source": [
    "You may input your own test text in the following script. Or, alternatively, you may choose a sample test text from our corpus, for example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23256f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text=ds_df.iloc[0][\"plain\"]\n",
    "print(test_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5192c92",
   "metadata": {},
   "source": [
    "Next, as the last step required before run, you need to specify a model.\n",
    "\n",
    "The models we used in our paper are:\n",
    "\n",
    "llama3.2:1b\n",
    "\n",
    "llama3.2:3b\n",
    "\n",
    "llama3.1:8b\n",
    "\n",
    "deepseek-r1:8b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a15c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelid=\"llama3.2:1b\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "583fedb8",
   "metadata": {},
   "source": [
    "Note: to use the model you specify, you'll need ollama installed and started. You may download Ollama here:\n",
    "\n",
    "https://ollama.com/\n",
    "\n",
    "And if ollama is not started, simply run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8809ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ollama start"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae7aeed1",
   "metadata": {},
   "source": [
    "Also, if you haven't download the model you specified, you may use the following script to download the model.\n",
    "\n",
    "(Here I use llama3.2:1b as an example.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f7125d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ollama pull llama3.2:1b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "901ee29e",
   "metadata": {},
   "source": [
    "Pack the chat of the sample text under prompt strategy, send chat to model for inferring, and retrieve result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efea9ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_strat=p_strats[pid]\n",
    "ct=copy.deepcopy(p_strat)\n",
    "ct[-1][\"content\"]=ct[-1][\"content\"].replace(\"[#TEST_TEXT]\",test_text)\n",
    "cr=chat(model=modelid, messages=ct)\n",
    "rs=cr.message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0771d1fe",
   "metadata": {},
   "source": [
    "View the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1803636",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "alpha",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
