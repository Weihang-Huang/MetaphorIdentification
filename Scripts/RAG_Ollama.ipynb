{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04dbab9c",
   "metadata": {},
   "source": [
    "# Metaphor Identification via RAG with Local Model via Ollama\n",
    "\n",
    "This notebook details the process of metaphor identification via RAG using the local model at Ollama. \n",
    "\n",
    "The following packages are needed to run this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066fa907",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas langchain ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba066f0",
   "metadata": {},
   "source": [
    "Import packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0cf447",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from langchain_core.documents import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import SKLearnVectorStore\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from ollama import chat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "240861b7",
   "metadata": {},
   "source": [
    "Please input a test text here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67fa972",
   "metadata": {},
   "outputs": [],
   "source": [
    "key_text=\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e26750a",
   "metadata": {},
   "source": [
    "Or, alternatively, you may choose a test text from our metaphor corpus.\n",
    "\n",
    "(Here, we choose the first text as an example.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42dd45d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_fp=\"data/metaphor_dataset.csv\"\n",
    "ds_df=pd.read_csv(ds_fp)\n",
    "ds_df=ds_df[[\"textid\",\"plain\"]]\n",
    "ds_df.rename(columns={\"plain\":\"context\"},inplace=True)\n",
    "\n",
    "key_text=ds_df.loc[0,\"context\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f2ceb1b",
   "metadata": {},
   "source": [
    "Load the metaphor protocol in plain text. This is used as the knowledge base for context retrival in the following process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7117e1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_context_fp=\"data/rag_context.txt\"\n",
    "with open(total_context_fp,\"r\",-1) as f:\n",
    "    total_context=f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4991e1e",
   "metadata": {},
   "source": [
    "Transform the text into Documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac91bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs=[[Document(page_content=text)] for text in [total_context]]\n",
    "docs_list=[item for sublist in docs for item in sublist]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2306e9",
   "metadata": {},
   "source": [
    "Split documents into chunks, on which the retrival is based upon. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e738d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter=RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=1000, chunk_overlap=0)\n",
    "doc_splits=text_splitter.split_documents(docs_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "143068c4",
   "metadata": {},
   "source": [
    "Vectorizing the split documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5acdb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = SKLearnVectorStore.from_documents(\n",
    "    documents=doc_splits,\n",
    "    embedding=OllamaEmbeddings(model=\"nomic-embed-text\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "408a0ba8",
   "metadata": {},
   "source": [
    "Construct a retriever, and use it to retrieve the context based on the inputted test text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26db5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever=vectorstore.as_retriever(k=1)\n",
    "retrieved_documents=retriever.invoke(key_text)\n",
    "context=retrieved_documents[0].page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38aef7c0",
   "metadata": {},
   "source": [
    "Construct the chat based on the retrieved context and the test text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4f12ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_strat=[\n",
    "    {\"role\":\"system\",\n",
    "     \"content\":\"You are a helpful AI assistant. Use the following pieces of context to answer the question at the end. If you don't know the answer, just say you don't know. DO NOT try to make up an answer. If the question is not related to the context, politely respond that you are tuned to only answer questions that are related to the context.\\n\"},\n",
    "    {'role': 'system',\n",
    "    'content': 'You are a linguistic expert trained in metaphor identification. When the user provides a text, follow this protocol:\\n• Identify all metaphorical expressions.\\n• Wrap each one in <Metaphor> and </Metaphor> tags.\\n• Reproduce the rest of the text exactly as written.\\n• Do not include any explanation, commentary, or extra content in this message.'},\n",
    "    {'role': 'user',\n",
    "    'content': 'Can you please identify and tag the metaphors in the following text?\\n'},\n",
    "]\n",
    "ct=p_strat.copy()\n",
    "p_strat[0][\"content\"]=p_strat[0][\"content\"]+context\n",
    "p_strat[-1][\"content\"]=p_strat[-1][\"content\"]+key_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8ece11",
   "metadata": {},
   "source": [
    "Next, as the last step required before run, you need to specify a model.\n",
    "\n",
    "The models we used in our paper are:\n",
    "\n",
    "llama3.2:1b\n",
    "\n",
    "llama3.2:3b\n",
    "\n",
    "llama3.1:8b\n",
    "\n",
    "deepseek-r1:8b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb43cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelid=\"llama3.2:1b\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dbcbdcc",
   "metadata": {},
   "source": [
    "Note: to use the model you specify, you'll need ollama installed and started. You may download Ollama here:\n",
    "\n",
    "https://ollama.com/\n",
    "\n",
    "And if ollama is not started, simply run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e238e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ollama start"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f138d2",
   "metadata": {},
   "source": [
    "Also, if you haven't download the model you specified, you may use the following script to download the model.\n",
    "\n",
    "(Here I use llama3.2:1b as an example.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22bc5393",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ollama pull llama3.2:1b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944eba57",
   "metadata": {},
   "source": [
    "Send chat to model for inferring, and retrieve result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e73cd81",
   "metadata": {},
   "outputs": [],
   "source": [
    "cr=chat(model=modelid, messages=ct)\n",
    "rs=cr.message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb63a0e",
   "metadata": {},
   "source": [
    "View the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84924e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "alpha",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
